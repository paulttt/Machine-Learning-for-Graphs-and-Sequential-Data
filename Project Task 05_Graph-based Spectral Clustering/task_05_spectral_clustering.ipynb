{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 05:  Clustering users based on their preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to find groups of users with similar preferences using **Spectral clustering**. \n",
    "You are given a fragment of the Yelp social network, represented by an undirected weighted graph.\n",
    "Nodes in the graph represent users.\n",
    "If two users are connected by an edge of weight $w$, it means that they have both left positive reviews to the same $w$ restaurants.\n",
    "\n",
    "Additionally, you are given a matrix `F` that encodes user preferences to different categories of restaurants. If `F[i, c] = 1`, then user `i` likes restaurants in category `c`.\n",
    "\n",
    "You are allowed to use the imported functions (`eigsh`, `KMeans`, `normalize`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "* `N` = number of users (nodes in the graph)\n",
    "* `C` = number of categories\n",
    "* The graph is stored as a sparse adjacency matrix `A` (shape `[N, N]`).\n",
    "* User preferences are stored in a matrix `F` (shape `[N, C]`). They will only be used for the final part of the assignment (Part 3)\n",
    "* Name of each category is provided in the list `categories` (length `[C]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.load_npz('A.npz')\n",
    "F = np.load('F.npy')\n",
    "categories = np.load('categories.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert A.shape[0] == F.shape[0]\n",
    "assert F.shape[1] == len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing spectral clustering\n",
    "## 1.1. Construct the graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to construct the Laplacian for the given graph. \n",
    "\n",
    "Given the **adjacency matrix** $A \\in \\mathbb{R}^{N \\times N},$ we define the **degree matrix** $D \\in \\mathbb{R}^{N \\times N}$ of an undirected graph as\n",
    "$$D_{ij} =  \\begin{cases}\\sum_{k=1}^N A_{ik} & if \\;\\; i = j\\\\ 0 & if \\;\\; i \\ne j\\end{cases}$$\n",
    "\n",
    "If our goal is to minimize the **ratio cut**, we will need to use the **unnormalized Laplacian**, defined as\n",
    "$$L_{unnorm} = D - A.$$\n",
    "\n",
    "If our goal is to minimze the **normalized cut**, we will need to use the **normalized Laplacian** (a.k.a. symmetrized Laplacian), defined as\n",
    "$$L_{sym} = I - D^{-1/2}AD^{-1/2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_laplacian(A, norm_laplacian):\n",
    "    \"\"\"Construct Laplacian of a graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    norm_laplacian : bool\n",
    "        Whether to construct the normalized graph Laplacian or not.\n",
    "        If True, construct the normalized (symmetrized) Laplacian, L = I - D^{-1/2} A D^{-1/2}.\n",
    "        If False, construct the unnormalized Laplacian, L = D - A.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    L : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Laplacian of the graph.\n",
    "        \n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    diags = A.sum(axis=1)\n",
    "    \n",
    "    if norm_laplacian:\n",
    "        diags =  1.0 / diags\n",
    "        D_inv_sqrt = sp.diags(np.ravel(diags), 0, (n,n)).sqrt()\n",
    "        L = sp.identity(n) - (D_inv_sqrt.dot(A)).dot(D_inv_sqrt)\n",
    "    else:\n",
    "        D = sp.diags(np.ravel(diags), 0, (n,n))\n",
    "        L = D - A\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Spectral embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to compute the spectral embedding for the given graph.\n",
    "\n",
    "In order to partition the graph into $k$ clusters, such that the desired cut (ratio or normalized) is minimized, we need to consider the $k$ eigenvectors corresponding to the $k$ smallest eigenvalues of the graph Laplacian.\n",
    "\n",
    "Since the Laplacian matrix is sparse and symmetric, we can use the function `eigsh` from the `scipy.sparse.linalg` package in order to find eigendecomposition of $L$ (`eig` - eigendecomposition, `s` - sparse, `h`- Hermitian).\n",
    "The function `eigsh` directly allows you to find the smallest / largest eigenvalues by specifying the `k` and `which` parameters. \n",
    "\n",
    "Keep in mind that the Laplacian matrix is always positive semi-definite when picking the appropriate value for the `which` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_embedding(A, num_clusters, norm_laplacian):\n",
    "    \"\"\"Compute spectral embedding of nodes in the given graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    num_clusters : int\n",
    "        Number of clusters to detect in the data.\n",
    "    norm_laplacian : bool, default False\n",
    "        Whether to use the normalized graph Laplacian or not.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    embedding : np.array, shape [N, num_clusters]\n",
    "        Spectral embedding for the given graph.\n",
    "        Each row represents the spectral embedding of a given node.\n",
    "    \n",
    "    \"\"\"\n",
    "    if (A != A.T).sum() != 0:\n",
    "        raise ValueError(\"Spectral embedding doesn't work if the adjacency matrix is not symmetric.\")\n",
    "    laplacian = construct_laplacian(A, norm_laplacian)\n",
    "    _, eigvecs = eigsh(laplacian, k=num_clusters, which=\"SM\")\n",
    "    return eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Determine the clusters based on the spectral embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use the K-means algorithm for assigning nodes to clusters, once the spectral embedding is computed.\n",
    "\n",
    "One thing you should keep in mind, is that when using the **normalized Laplacian**, the rows of the embedding matrix **have to** be normalized to have unit $L_2$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(A, num_clusters, norm_laplacian):\n",
    "    \"\"\"Perform spectral clustering on the given graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    num_clusters : int\n",
    "        Number of clusters to detect in the data.\n",
    "    norm_laplacian : bool, default False\n",
    "        Whether to use the normalized graph Laplacian or not.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    z_pred : np.array, shape [N]\n",
    "        Predicted cluster indicators for each node.\n",
    "        \n",
    "    \"\"\"\n",
    "    embedding = spectral_embedding(A, num_clusters, norm_laplacian)\n",
    "    if norm_laplacian:\n",
    "        embedding = normalize(embedding)\n",
    "    z_pred = KMeans(n_clusters=num_clusters).fit_predict(embedding)\n",
    "    return z_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quantitatively evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_list_of_clusters(z):\n",
    "    \"\"\"Convert predicted label vector to a list of clusters in the graph.\n",
    "    This function is already implemented, nothing to do here.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.array, shape [N]\n",
    "        Predicted labels.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list_of_clusters : list of lists\n",
    "        Each list contains ids of nodes that belong to the same cluster.\n",
    "        Each node may appear in one and only one partition.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> z = np.array([0, 0, 1, 1, 0])\n",
    "    >>> labels_to_list_of_clusters(z)\n",
    "    [[0, 1, 4], [2, 3]]\n",
    "    \n",
    "    \"\"\"\n",
    "    return [np.where(z == c)[0] for c in np.unique(z)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Compute ratio cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement functions for computing the **ratio cut** and **normalized cut** for a given partition.\n",
    "\n",
    "Ratio cut and normalized cut are defined on the slide 14 of the lecture slides.\n",
    "\n",
    "\n",
    "The function `labels_to_list_of_clusters` can be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratio_cut(A, z):\n",
    "    \"\"\"Compute the ratio cut for the given partition of the graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    z : np.array, shape [N]\n",
    "        Cluster indicators for each node.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ratio_cut : float\n",
    "        Value of the cut for the given partition of the graph.\n",
    "        \n",
    "    \"\"\"\n",
    "    cluster_list = labels_to_list_of_clusters(z)\n",
    "    ratio_cut = 0\n",
    "    for cluster in cluster_list:\n",
    "        normalizer = len(cluster)\n",
    "        not_cluster = np.setdiff1d(range(len(z)), cluster)\n",
    "        ratio_cut += (A[cluster, :][: ,not_cluster]).sum() / normalizer\n",
    "    return ratio_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Compute normalized cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: if a cluster only contains a single node, define its volume to be 1 to avoid division by zero errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_cut(A, z):\n",
    "    \"\"\"Compute the normalized cut for the given partition of the graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scipy.sparse.csr_matrix, shape [N, N]\n",
    "        Adjacency matrix of the graph.\n",
    "    z : np.array, shape [N]\n",
    "        Cluster indicators for each node.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    norm_cut : float\n",
    "        Value of the normalized cut for the given partition of the graph.\n",
    "        \n",
    "    \"\"\"\n",
    "    cluster_list = labels_to_list_of_clusters(z)\n",
    "    norm_cut = 0\n",
    "    for cluster in cluster_list:\n",
    "        if len(cluster) == 1:\n",
    "            vol = 1\n",
    "        else:\n",
    "            vol = np.sum(np.ravel(A.sum(axis=1))[cluster])\n",
    "        not_cluster = np.setdiff1d(range(len(z)), cluster)\n",
    "        norm_cut += (A[cluster, :][:, not_cluster]).sum() / vol\n",
    "    return norm_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, how using the unnormalized Laplacian leads to a much better ratio cut, while the normalized Laplacian leads to better normalized cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using L_unnorm:\n",
      " ratio cut = 369.109\n",
      " normalized cut = 369.000\n",
      " sizes of partitions are: [3379, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "norm_laplacian = False\n",
    "z_unnorm = spectral_clustering(A, num_clusters, norm_laplacian)\n",
    "print('When using L_unnorm:')\n",
    "print(' ratio cut = {:.3f}'.format(compute_ratio_cut(A, z_unnorm)))\n",
    "print(' normalized cut = {:.3f}'.format(compute_normalized_cut(A, z_unnorm)))\n",
    "print(' sizes of partitions are: {}'.format([len(clust) for clust in labels_to_list_of_clusters(z_unnorm)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using L_norm:\n",
      " ratio cut = 5942.851\n",
      " normalized cut = 4.343\n",
      " sizes of partitions are: [572, 350, 742, 754, 577, 389]\n"
     ]
    }
   ],
   "source": [
    "norm_laplacian = True\n",
    "z_norm = spectral_clustering(A, num_clusters, norm_laplacian)\n",
    "print('When using L_norm:')\n",
    "print(' ratio cut = {:.3f}'.format(compute_ratio_cut(A, z_norm)))\n",
    "print(' normalized cut = {:.3f}'.format(compute_normalized_cut(A, z_norm)))\n",
    "print(' sizes of partitions are: {}'.format([len(clust) for clust in labels_to_list_of_clusters(z_norm)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of the assignment, your task is to print out the 5 most popular types of restaurants visited by the users in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_categories_for_each_cluster(top_k, z, F, categories):\n",
    "    \"\"\"Print the top-K categories among users in each cluster.\n",
    "    For each cluster, the function prints names of the top-K categories,\n",
    "    and number of users that like the respective category (separated by a comma).\n",
    "    The function doesn't return anything, just prints the output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    top_k : int\n",
    "        Number of most popular categories to print for each cluster.\n",
    "    z : np.array, shape [N]\n",
    "        Cluster labels.\n",
    "    F : sp.csr_matrix, shape [N, C]\n",
    "        Matrix that tells preferences of each user to each category.\n",
    "        F[i, c] = 1 if user i gave at least one positive review to at least one restaurant in category c.\n",
    "    categories : list, shape [C]\n",
    "        Names of the categories.\n",
    "        \n",
    "    \"\"\"\n",
    "    clusters = labels_to_list_of_clusters(z)\n",
    "    for cluster_num, cluster in enumerate(clusters):\n",
    "        category_cluster_scores = F[cluster,:].sum(axis=0)\n",
    "        most_popular_categories = np.argsort(category_cluster_scores)[-top_k:]\n",
    "        print(\"Most popular categories in cluster {}\".format(cluster_num))\n",
    "        for i in range(top_k-1, -1, -1):\n",
    "            print(\"\\t {}, {}\".format(categories[most_popular_categories[i]], category_cluster_scores[most_popular_categories[i]]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular categories in cluster 0\n",
      "\t Specialty Food, 356.0\n",
      "\t Thai, 355.0\n",
      "\t Breakfast & Brunch, 348.0\n",
      "\t Japanese, 333.0\n",
      "\t Ethnic Food, 330.0\n",
      "\n",
      "\n",
      "Most popular categories in cluster 1\n",
      "\t Seafood, 316.0\n",
      "\t Mexican, 315.0\n",
      "\t Sandwiches, 295.0\n",
      "\t Japanese, 291.0\n",
      "\t Breakfast & Brunch, 286.0\n",
      "\n",
      "\n",
      "Most popular categories in cluster 2\n",
      "\t Breakfast & Brunch, 665.0\n",
      "\t Italian, 625.0\n",
      "\t American (Traditional), 518.0\n",
      "\t Sandwiches, 518.0\n",
      "\t Pizza, 484.0\n",
      "\n",
      "\n",
      "Most popular categories in cluster 3\n",
      "\t Japanese, 535.0\n",
      "\t Chinese, 447.0\n",
      "\t Asian Fusion, 417.0\n",
      "\t Sushi Bars, 414.0\n",
      "\t Desserts, 411.0\n",
      "\n",
      "\n",
      "Most popular categories in cluster 4\n",
      "\t Japanese, 499.0\n",
      "\t Breakfast & Brunch, 456.0\n",
      "\t Sandwiches, 429.0\n",
      "\t Italian, 414.0\n",
      "\t Asian Fusion, 409.0\n",
      "\n",
      "\n",
      "Most popular categories in cluster 5\n",
      "\t Breakfast & Brunch, 636.0\n",
      "\t Sandwiches, 528.0\n",
      "\t Italian, 514.0\n",
      "\t Pizza, 482.0\n",
      "\t Coffee & Tea, 473.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "z_norm = spectral_clustering(A, num_clusters, True)\n",
    "r = print_top_categories_for_each_cluster(5, z_norm, F, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
